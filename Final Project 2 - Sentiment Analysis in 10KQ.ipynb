{"cells":[{"cell_type":"markdown","metadata":{"id":"9_qjaIAI1wTU"},"source":["This project ask you to perform sentiment analysis on 10-K Financial Reports.\n","\n","1. You are asked to pick 1 firm, download the entire history of 10-K and 10-Q financial reports from the SEC Edgar Server.\n","\n","2. Perform sentiment analysis on all the 10-K and 10-Q reports.\n","\n","3. Merge with stock return data.\n","\n","4. Determine if changes in negative sentiment can predict stock returns."]},{"cell_type":"markdown","metadata":{"tags":[],"id":"uHHcgJQo1wTZ"},"source":["# Final Project: Sentiment Analysis of 10-K and 10-Q"]},{"cell_type":"markdown","metadata":{"id":"y7AzJ1b31wTZ"},"source":["## Obtain 10-K Financial Reports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHlt4cTK1wTa","executionInfo":{"status":"ok","timestamp":1685319046416,"user_tz":300,"elapsed":13281,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}},"outputId":"2ec81385-b3b8-4cb0-ea30-5731990d731a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sec-edgar-downloader\n","  Downloading sec_edgar_downloader-4.3.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sec-edgar-downloader) (2.27.1)\n","Collecting bs4 (from sec-edgar-downloader)\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sec-edgar-downloader) (4.9.2)\n","Collecting Faker (from sec-edgar-downloader)\n","  Downloading Faker-18.9.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->sec-edgar-downloader) (4.11.2)\n","Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker->sec-edgar-downloader) (2.8.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sec-edgar-downloader) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sec-edgar-downloader) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->sec-edgar-downloader) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sec-edgar-downloader) (3.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker->sec-edgar-downloader) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->sec-edgar-downloader) (2.4.1)\n","Building wheels for collected packages: bs4\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=3087c212f5b8a6cfd7784cc5514aa9f99f2e586ff7dda7c84e402e50e720186f\n","  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n","Successfully built bs4\n","Installing collected packages: Faker, bs4, sec-edgar-downloader\n","Successfully installed Faker-18.9.0 bs4-0.0.1 sec-edgar-downloader-4.3.0\n"]}],"source":["pip install -U sec-edgar-downloader"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"D-w2qZhT1wTc","executionInfo":{"status":"ok","timestamp":1685319046901,"user_tz":300,"elapsed":494,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["from sec_edgar_downloader import Downloader\n","dl = Downloader(\"/Users/carlotaginer/Downloads/\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsMy2o2J1wTc","executionInfo":{"status":"ok","timestamp":1685319724508,"user_tz":300,"elapsed":68999,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}},"outputId":"8f31a5d0-73be-4407-e39c-506790a39db9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Skipping filing detail download for '0001032210-00-001961' due to network error: 404 Client Error: Not Found for url: https://www.sec.gov/Archives/edgar/data/789019/000103221000001961/0001.txt.\n"]},{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":3}],"source":["dl.get(\"10-K\", \"MSFT\")"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"XCdrWzKV1wTd"},"source":["## Analysis \n","\n","Run tests to examine whether changes in sentiment predicts future stock returns.\n","\n","Cumulative 5 days, 10 days, 1 month returns\n","    \n","Negative sentiment vs positive sentiment vs overall sentiment\n","        "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"dd8F4VvR1wTd","executionInfo":{"status":"error","timestamp":1685319736483,"user_tz":300,"elapsed":498,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}},"outputId":"f384c2fc-6a61-4612-997f-a4597314467c"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-90a0d974d6ea>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Opening the company file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"company.idx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#confirming the header of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'company.idx'"]}],"source":["# Opening the company file\n","index_file = open(\"company.idx\").readlines()\n","\n","#confirming the header of the file\n","print(index_file[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isLz-sFS1wTd","executionInfo":{"status":"aborted","timestamp":1685319724512,"user_tz":300,"elapsed":284,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["# I will keep the information from this line in a list called find_list\n","\n","find_list = []\n","item = 0\n","line = 0\n","\n","while item < 100:\n","i = index_file[line]\n","loc1 = i.find('10-K')\n","loc2 = i.find(\"NT 10-K\") \n","loc3 = i.find(\"10-K/A\")\n","\n","#I will strictly keep 10-K files, not NT 10-K or 10-K/A\n","if (loc2 == -1) and (loc1 != -1) and (loc3 == -1):\n","    find_list.append(i)\n","line+=1\n","item = len(find_list)\n","\n","# Sanity check: if the command worked properly, the list should have 100 items.\n","print(len(find_list))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"Mg2aBnw_1wTe","executionInfo":{"status":"error","timestamp":1685319779689,"user_tz":300,"elapsed":559,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}},"outputId":"f7831940-da9c-43f5-9dd6-2df49a038db8"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-920f934e105f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mReportList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCompany_No\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msplit_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mReportList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.sec.gov/Archives/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'find_list' is not defined"]}],"source":["# commands below will split the line, and send the links to a list called ReportList, and the CIK+date issued to a list called Company_No (this will be the name of the file when download)\n","ReportList = []\n","Company_No = []\n","for i in find_list:\n","    split_i = i.split()\n","    ReportList.append(\"https://www.sec.gov/Archives/\" + split_i[-1])\n","    Company_No.append(split_i[-3] + \"_\" + split_i[-2])\n","print(ReportList[:5])\n","print(Company_No[:5])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO3PCZpD1wTe","executionInfo":{"status":"aborted","timestamp":1685319724516,"user_tz":300,"elapsed":279,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["import os\n","os.chdir(\"C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/10K Files\")\n","\n","def createfile(filename, content):\n","    name= filename + \".txt\"  # Here I define the name of the file\n","    with open(name, \"w\") as file:\n","        file.write(str(content)) # Here I define its content, which will be the textual content from the 10-K files.\n","        file.close()\n","        print(\"Succeed!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cymTykf1wTf","executionInfo":{"status":"aborted","timestamp":1685319724517,"user_tz":300,"elapsed":276,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["company_order = 0\n","unable_request = 0\n","\n","for a_index in range(100):\n","    web_add = ReportList[a_index]\n","    filename = Company_No[a_index]\n","\n","    webpage_response = requests.get(web_add, headers={'User-Agent': 'Mozilla/5.0'}) \n","\n","    if webpage_response.status_code == 200: \n","        # The HTTP 200 OK success status response code indicates that the request has succeeded. \n","        body = webpage_response.content\n","        createfile(filename, body)\n","    else:\n","        print (\"Unable to get response with Code : %d \" % (webpage_response.status_code))\n","        unable_request += 1\n","\n","    a_index +=1\n","\n","print(unable_request) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Q0lCeJ21wTf","executionInfo":{"status":"aborted","timestamp":1685319724518,"user_tz":300,"elapsed":275,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":[" # First, I will create a list with the name of the files in the folder where I downlaoded the 10-K files\n","os.chdir(\"C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/10K Files\")\n","all_files = os.listdir()\n","\n","# I will use only 10 random files from the 100 we downloaded.\n","import random\n","\n","# I create a list with 10 random indexes between 0 and 100 to selected the files in the folder.\n","random_index = []\n","while len(random_index) < 10:\n","    n = random.randint(0,100)\n","    if n not in random_index:\n","        random_index.append(n)\n","\n","# Here I create a new list with the 10 chosen files.\n","chosen_file = [all_files[i] for i in random_index]\n","print(chosen_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwGys1-U1wTg","executionInfo":{"status":"aborted","timestamp":1685319724520,"user_tz":300,"elapsed":273,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["def read_txt(file_name):\n","    txt_file = open(file_name,\"r\",encoding='UTF8')                                       \n","    str_txt = txt_file.read()\n","    return str_txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLNKe1fD1wTg","executionInfo":{"status":"aborted","timestamp":1685319724522,"user_tz":300,"elapsed":273,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["\n","import re\n","text_start_pattern = re.compile(r'<DOCUMENT>') \n","text_end_pattern = re.compile(r'</DOCUMENT>')\n","type_pattern = re.compile(r'<TYPE>10-K[^\\n]+')\n","\n","# Here I will define a function that will be used to extract the textual data from 10-K txt files.\n","def textual_content(file):\n","    doc_start_list = [x.start() for x in text_start_pattern.finditer(file)] #assigns the first index from the starting pattern created before\n","    oc_end_list = [x.end() for x in text_end_pattern.finditer(file)] #assigns the last index from the ending pattern created before\n","    type_list = [x[len('<TYPE>'):] for x in type_pattern.findall(file)] #assigns the type of the documents, which will always be 10-K's because we restricted it before\n","    for doc_type, start_index, end_index in zip(type_list, doc_start_list, doc_end_list):\n","        report_content = file[start_index:end_index]\n","    return report_content\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZMrTAUV1wTg","executionInfo":{"status":"aborted","timestamp":1685319724524,"user_tz":300,"elapsed":273,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["text_initial = read_txt(chosen_file[0])\n","text_10k = textual_content(text_initial)\n","print(text_10k[0:30])\n","print(text_10k[-30:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cEKd2cx1wTg","executionInfo":{"status":"aborted","timestamp":1685319724525,"user_tz":300,"elapsed":273,"user":{"displayName":"Carlota Giner","userId":"12569826331451570461"}}},"outputs":[],"source":["from bs4 import BeautifulSoup\n","def BeautifulSoup_clean1(str_txt):\n","    soup = BeautifulSoup(str_txt,'html.parser')\n","    return soup.get_text()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDoIUEaJ1wTh"},"outputs":[],"source":["import string\n","from nltk.tokenize import word_tokenize\n","\n","def further_clean(text):\n","    \n","\n","# This function replaces it by an empty space on the text \n","    for a_sign in ['\\\\n', '\\\\t', '☐', '☒', '\\xa0', '●', '“', '”']:\n","        text = text.replace(a_sign,\" \")\n","\n","# Here, for each punctution in a set of all existing punctuations, the function also replaces it by an empty space.\n","    for a_punc in string.punctuation:\n","        text = text.replace(a_punc, \" \")\n","\n","\n","text = re.sub('\\s+',\" \", text).lower()\n","\n","return text.strip() \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGQae-mK1wTh"},"outputs":[],"source":[" # Tokenizing and counting the total frequency of words\n","\n","from nltk.tokenize import word_tokenize\n","\n","def word_count(text):\n","    word_list = word_tokenize(text) #splits the text into words and assigns it to a new list\n","    total_num = len(word_list) #count the number of words in the new list\n","    return total_num, word_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfMFRC-d1wTi","outputId":"ed8bc319-489b-4746-fb5b-a883b7e21d2e"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/LM_negative_wordlist.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/xl/p4rch97d35b5br4pw0s4ljlm0000gn/T/ipykernel_2984/1635532068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLM_negw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/LM_negative_wordlist.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Transforming the file into a list of negative words, and converting all entries to lowercase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/LM_negative_wordlist.csv'"]}],"source":["import pandas as pd \n","\n","LM_negw = pd.read_csv(\"C:/Users/carlotaginer/Desktop/Open Code/10K Text Mining/LM_negative_wordlist.csv\")\n","\n","# Transforming the file into a list of negative words, and converting all entries to lowercase.\n","LM_negw = list(LM_negw['Negative_word'])\n","LM_negw = [j.lower() for j in LM_negw]\n","\n","# I will create a dictionary that assigns the number of appearances to each negative words\n","\n","negw_dic = {} #creates an empty dictionary\n","for A_NegWord in LM_negw:\n","    negw_dic[A_NegWord] = 0 # For each word in the LM_negw list, we create an entry inside the dictonary with the word and the number 0.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTO95cqJ1wTi"},"outputs":[],"source":["# I will redefine the function to tokenize the text and will add the steps to count the negative words.\n","\n","from nltk.tokenize import word_tokenize\n","\n","def word_count(text):\n","\n","    # Tokenizing and counting total words\n","    word_list = word_tokenize(text) # Splits the text into words and assigns it to a new list\n","    total_num = len(word_list) # Count the number of words in the new list\n","\n","\n","    # Counting negative words\n","\n","    negw_dic_ex = negw_dic.copy() # This commands creates a copy of the dictionary we created with the L&M negative word list\n","\n","    # This loops checks if each word in the text is a negative word, and if so, adds 1 to the respective entry in \n","    # the \"negw_dic_ex\" dictionary.\n","\n","    for a_word in word_list:\n","        if a_word in LM_negw:\n","            negw_dic_ex[a_word] += 1\n","\n","\n","    # This commands creates a new variable from a dictionary (the negw_dic_ex created before), and the orient=\"index\" means that\n","    # the keys of the dictionary will be the rows of the variable.\n","\n","    negw_df = pd.DataFrame.from_dict(negw_dic_ex, orient='index')\n","    negw_df.reset_index(inplace=True) # This command modifies the indexes of the list, converting them to default indexes.\n","    # This command renames the columns of the variable (inplace=True means that the variable will be modified without creating a new variable)\n","    negw_df.rename(columns={\"index\": \"Negative_words\", 0: \"Word_counts\"}, inplace=True)\n","\n","\n","    total_neg = sum(list(negw_dic_ex.values()))  # Total number of negative words\n","    negw_fre = total_neg/total_num # % of negative words in the text\n","\n","    return {\"Negative_WordCount_df\":negw_df, \"Total_WordNumber\":total_num, \"Total_Negative_WordCount\": total_neg,\n","        \"Negative_WordFrequency\": negw_fre}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfQev79A1wTj"},"outputs":[],"source":["# First, we open 4 empty lists to append the 4 returns from the function we've just defined.\n","Neg_WordDf_list = []\n","Total_Word_list = []\n","Total_Neg_Word_list = []\n","Frequency_list = []\n","\n","# For each file in the chosen_file list (the list with the 10-K txt files)\n","for a_file in chosen_file:\n","    text = read_txt(a_file) #run the read_txt fuction that opens the txt file\n","    text = textual_content(text) #run the textual_content function to get the textual content from the file opened before.\n","    text = BeautifulSoup_clean1(text) #run the BeautifulSoup_clean1 function to clean the data even better using the html.parser algorithm. \n","    text = further_clean(text) #run the further_clean function to remove symbols, punctiations, and convert strings to lower case. \n","    result = word_count(text) #run the word_count functions that tokenize the text, and deal with negative word counting.\n","\n","    Neg_WordDf_list.append(result['Negative_WordCount_df'])\n","    Total_Word_list.append(result[\"Total_WordNumber\"])\n","    Total_Neg_Word_list.append(result[\"Total_Negative_WordCount\"])\n","    Frequency_list.append(result[\"Negative_WordFrequency\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTZkFWqc1wTj"},"outputs":[],"source":["# Here we create a new dictionary using data from the lists defined before.\n","summary = {'File_name':chosen_file, 'Total_WordNumber':Total_Word_list, 'Total_Negative_WordCount':Total_Neg_Word_list,'Negative_WordFrequency':Frequency_list}\n","\n","summary_df = pd.DataFrame(summary, index=range(1,11))\n","summary_df"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}